# Tutorial: Training using a data frame generated by EasyVVUQ

In this tutorial we will create a forward uncertainty-propagation surrogate using a Deep Active Subspace network, trained on a data frame generated by [EasyVVUQ](https://github.com/UCL-CCS/EasyVVUQ). Forward uncertainty propagation is defined as computing the output distribution of a computational model, given *assumed* probability density functions for the input paramaters of the model, see the image below for a sketch of the problem. A related problem is creating a cheap surrogate model for the input-output map, which can be evaluated at a fraction of the cost of the computational model. EasyVVUQ is VECMA's forward uncertainty propagation toolkit. 

![](forward.png)

## Problem definition

For computational efficiency we will consider the analytical Sobol G function, see [this page](https://www.sfu.ca/~ssurjano/gfunc.html) for a desription of this function. The following files are relevant:

* `tests/deep_active_subspaces/model/g_func.py`: the implementation of the Sobol G function.
* `tests/deep_active_subspaces/generate_easyvvuq_dataframe.py`: script that generates the training data using EasyVVUQ.
* `tests/deep_active_subspaces/train_DAS_surrogate.py`: script that trains a Deep Active Subspace (DAS) surrogate on the EasyVVUQ data frame.

## Active subspaces

The Active Subspace method, introduced in [1], is a class of methods for forward propagation of uncertainty in high-dimensional input spaces. It attempts to circumvent the curse of dimensionality by
dimension reduction of the input space. Specifically, it projects the input vector x (of size `D`) to a lower-dimensional subspace y of size `d` (`d < D`), via a tall-and-skinny matrix `W_1` of orthogonal basis vectors. The active subspace is thus given by

![equation](https://latex.codecogs.com/svg.latex?%7B%5Cbf%20y%7D%20%3D%20W_1%5ET%7B%5Cbf%20x%7D%20%5Cin%5Cmathbb%7BR%7D%5Ed%2C)

where the main idea is that the dimension reduction simplifies the task of obtaining an accurate surrogate model. If we denote this surrogate by `g(y)`, and the full code as `f(x)`, we thus want to find a model that satisfies

![equation](https://latex.codecogs.com/svg.latex?f%28%7B%5Cbf%20x%7D%29%5Capprox%20g%5Cleft%28%7B%5Cbf%20y%7D%5Cright%29%20%3D%20g%5Cleft%28W_1%5ET%7B%5C%20%5Cbf%20x%7D%5Cright%29.)

Because `y` is a linear transformation of the inputs `x`, it opens up the possibility of findings directions along which the model varies most. This is especially useful if a model varies significantly in a direction that is not aligned with the coordinate axes of `x`. In `classical' active subspaces [1], dimension reduction is achieved by rotating the coordinate system such that it is aligned with the directions of most variability, after which only the most dominant directions are retained. To find these directions, the following average gradient matrix is constructed:

![equation](https://latex.codecogs.com/svg.latex?C%20%3D%20%5Cint%20%5Cleft%28%5Cnabla%20f%5Cleft%28%7B%5Cbf%20x%7D%5Cright%29%5Cright%29%5Cleft%28%5Cnabla%20f%5Cleft%28%7B%5Cbf%20x%7D%5Cright%29%5Cright%29%5ET%20p%28%5Cbf%20x%29d%7B%5Cbf%20x%7D)

Here, `p(x)` is the chosen probability density function (pdf) of the inputs `x`. Since `C` is a symmetric, positive semi-definite matrix, it has the following spectral decomposition

![equation](https://latex.codecogs.com/svg.latex?C%20%3D%20%5BW_1%20W_2%5D%5Cleft%5B%20%5Cbegin%7Bmatrix%7D%20%5CLambda_1%20%26%200%20%5C%5C%200%20%26%20%5CLambda_2%20%5Cend%7Bmatrix%7D%20%5Cright%5D%5BW_1%20W_2%5D%5ET.)

Hence the matrix `W_1`, used to project `x` to `y` are the first `d` eigenvectors of `C`, which in turn correspond to the `d` largest eigenvalues. These `d` eigenvectors form an orthonormal basis aligned with the directions of most variability. Note that `C` is averaged over `p(x)`, and that in practise, the integral in `C` is often approximated using a Monte Carlo approach.

The approach described here is intuitive, and has nice theoretical properties, such as computable error bounds. The downside however, is that the gradient of `f` must be available. This requires the availability of an adjoint solver, or one must approximate the gradients using for instance finite differences. This downside has prompted the development of other active subspace methods which do not require access to the gradient. Some of these methods involve Gaussian processes [2], whereas others use deep learning. We will focus on the latter.

## Deep Active Subspaces

In [3], an approach is described in which artificial neural networks (ANNs) are used for `g`, and where `W_1` is found using back propagation. The column vectors of `W_1` still form an orthogonal basis. The difference is that these column vectors are no longer the eigenvectors of the gradient matrix `C`, but instead are constructed using Gram-Schmidt orthogonalization. As such, `W_1` is parametrized by a matrix `Q` of the same dimension, where the non-orthonormal column vectors `q_i` (size D) are made orthonormal via:

![equation](https://latex.codecogs.com/svg.latex?%7B%5Cbf%20w%7D_i%20%3D%20%7B%5Cbf%20q%7D_i%20-%20%5Csum_%7Bj%3D1%7D%5E%7Bi-1%7D%5Cleft%28%5Cfrac%7B%7B%5Cbf%20w%7D_j%5ET%7B%5Cbf%20q%7D_i%7D%7B%7B%5Cbf%20w%7D_j%5ET%7B%5Cbf%20w%7D_j%7D%5Cright%29%7B%5Cbf%20w%7D_j%2C%20%5Cquad%20i%20%3D%201%2C%5Ccdots%2C%20d.)

That is, we start with $\wb_1 := \qb_1$, and for all subsequent vectors $\qb_i$ we subtract the projections of $\qb_i$ onto each vector $\wb_j$ which has previously been orthogonalized. This leaves us with a orthogonal basis:

![equation](https://latex.codecogs.com/svg.latex?%5Cleft%5B%7B%5Cbf%20w%7D_1%28%7B%5Cbf%20q%7D_1%29%5C%3B%5C%3B%7B%5Cbf%20w%7D_2%28%7B%5Cbf%20q%7D_1%2C%20%7B%5Cbf%20q%7D_2%29%5C%3B%5C%3B%5Ccdots%5C%3B%5C%3B%7B%5Cbf%20w%7D_d%28%7B%5Cbf%20q%7D_1%2C%7B%5Cbf%20q%7D_2%5C%2C%5Ccdots%2C%7B%5Cbf%20q%7D_d%29%5Cright%5D)

Finally, to obtain an orthonormal basis, each column vector is divided by its length. such that our final weight matrix becomes

![equation](https://latex.codecogs.com/svg.latex?W_1%28Q%29%20%3D%20%5Cleft%5B%5Cfrac%7B%7B%5Cbf%20w%7D_1%28%7B%5Cbf%20q%7D_1%29%7D%7B%5ClVert%7B%5Cbf%20w%7D_1%28%7B%5Cbf%20q%7D_1%29%5CrVert_2%7D%5C%3B%5C%3B%5Cfrac%7B%7B%5Cbf%20w%7D_2%28%7B%5Cbf%20q%7D_1%2C%20%7B%5Cbf%20q%7D_2%29%7D%7B%5ClVert%7B%5Cbf%20w%7D_2%28%7B%5Cbf%20q%7D_1%2C%20%7B%5Cbf%20q%7D_2%29%5CrVert_2%7D%5C%3B%5C%3B%5Ccdots%5C%3B%5C%3B%5Cfrac%7B%7B%5Cbf%20w%7D_d%28%7B%5Cbf%20q%7D_1%2C%7B%5Cbf%20q%7D_2%5C%2C%5Ccdots%2C%7B%5Cbf%20q%7D_d%29%7D%7B%5ClVert%7B%5Cbf%20w%7D_d%28%7B%5Cbf%20q%7D_1%2C%7B%5Cbf%20q%7D_2%5C%2C%5Ccdots%2C%7B%5Cbf%20q%7D_d%29%5CrVert_2%7D%5Cright%5D.)

Note that the projection `y = W_1^Tx` also occurs in a layer of a neural network if the activation function is linear. Thus, we can interpret the matrix `W_1` as a weight matrix of the first hidden layer (with `d` neurons and linear activation), connected to an input layer through which `x` is passed. Each column vector `w_i` contains the all weights connecting the input layer to the i-th neuron of the first hidden layer, see Figure below. Since the first hidden layer has only `d` neurons, and its weight matrix is determined from a Gram-Schmidt procedure, we call the layer the Deep Active Subspace (DAS) layer.

![](nn.png)

The surrogate `g(y)` is the ANN from the DAS layer onward, see Figure below. Each hidden layer has a weight matrix `W_i`. As per usual, these weight matrices are optimized through the back propagation algorithm, in which the gradient `dL/dW_i` is computed, where `L` is the loss function.

![](nn2.png)

The situation in the DAS layer is different. Since `W_1=W_1(Q)`, we need to optimize `Q` instead of the weight matrix, and therefore back propagation requires `dL/dQ` instead of `dL/dW_1`. The authors of [3] suggest to use automatic differentiation. This does make sense, since although Gram-Schmidt vectors are algebraic and differentiable, it quickly becomes a complicated expression involving a very large number of `q_ij` terms. Here, `q_ij` are the `D` entries of column vector `q_j`. That said, we used matrix calculus to find a simple expression for `dL/dQ`. Details are given in the attached `report_DAS.pdf` file.

This concludes the theoretical background. For more information, we refer to [3].

## EasyVVUQ Latin Hypercube campaign

We will use EasyVVUQ to generate a Latin Hypercube sampling plan of the Sobol G function in `D` dimensions. We will assume you are familiar with EasyVVUQ, and copy the code of `tests/deep_active_subspaces/generate_easyvvuq_dataframe.py` wholesale below:

```python
# the absolute path of this file
HOME = os.path.abspath(os.path.dirname(__file__))

# EasyVUQ work directory
WORK_DIR = '/tmp'

# EasyVVUQ database location
ID = 'g_func'
DB_LOCATION = "sqlite:///" + WORK_DIR + "/campaign%s.db" % ID

########################
# EasyVVUQ MC Campaign #
########################

# choose a number of uncertain parameters (< 10)
D = 2

# Define parameter space
params = {}
for i in range(10):
    params["x%d" % (i + 1)] = {"type": "float",
                               "min": 0.0,
                               "max": 1.0,
                               "default": 0.5}
params["D"] = {"type": "integer", "default": D}
params["out_file"] = {"type": "string", "default": "output.csv"}
output_filename = params["out_file"]["default"]
output_columns = ["f"]

# the a vector determines the importance of each input
a = np.zeros(10)
a[0] = 1.0; a[1] = 1.0
for i in range(10):
    params["a%d" % (i + 1)] = {"type": "float",
                               "min": 0.0,
                               "max": 1.0,
                               "default": a[i]}

# create encoder, decoder, and execute locally
encoder = uq.encoders.GenericEncoder(template_fname=HOME + '/model/g_func.template',
                                     delimiter='$',
                                     target_filename='in.json')
decoder = uq.decoders.SimpleCSV(target_filename=output_filename,
                                output_columns=output_columns)
execute = ExecuteLocal('{}/model/g_func.py in.json'.format(os.getcwd()))
actions = Actions(CreateRunDirectory(root=WORK_DIR),
                  Encode(encoder), execute, Decode(decoder))

# uncertain variables
vary = {}
for i in range(D):
    vary["x%d" % (i + 1)] = cp.Uniform(0, 1)

# MC sampler
my_sampler = uq.sampling.quasirandom.LHCSampler(vary=vary, max_num=1000)

# EasyVVUQ Campaign
campaign = uq.Campaign(name='g_func', params=params, actions=actions,
                       work_dir=WORK_DIR, db_location=DB_LOCATION)

# Associate the sampler with the campaign
campaign.set_sampler(my_sampler)

# Execute runs
campaign.execute().collate()
```

We highlight the `a` vector of the code above. This vector, of size `D`, determines the importance of each input. Entries are positive and higher values indicates less significant parameters.

## Training a DAS surrogate on the EasyVVUQ data frame

The EasyVVUQ data frame can be read into EasySurrogate via:

```python

# reload EasyVVUQ campaign
campaign = uq.Campaign(name=ID, db_location=DB_LOCATION)
print("===========================================")
print("Reloaded campaign {}".format(ID))
print("===========================================")
sampler = campaign.get_active_sampler()
campaign.set_sampler(sampler, update=True)

# Create an EasySurrogate campaign
surr_campaign = es.Campaign()

# This is the main point of this test: extract training data from EasyVVUQ data frame
features, samples = surr_campaign.load_easyvvuq_data(campaign, qoi_cols='f')
```

Here, `DB_LOCATION` is the location of the EasyVVUQ database, which we stored in the work directory of the EasyVVUQ campaign. This will output something like:

```
Extracting features ['x1', 'x2', 'x3', 'x4', 'x5']
Extracting output data ['f'] 
```
indicating that it has read samples drawn from 5 input variables, which will be used as features. The output data is in this case a single column of the CSV output file of the G function, named '`f`. If you wish to read multiple columns, specify a list of names under the `qoi_cols` parameter of `load_easyvvuq_data`. Note that `features` is an array, wheres `samples` is a dictionary indexed by the `qoi_cols`. In this case, `samples['f']` will return an array with all output values of the G function. We will use the data to train a DAS surrogate, (see `tests/deep_active_subspaces/train_DAS_surrogate.py`). This is done via:

```python
# create DAS surrogate object
surrogate = es.methods.DAS_Surrogate()
# train the DAS surrogate
surrogate.train(params, samples, 
                d, n_iter=10000, n_layers=4, n_neurons=100, 
                test_frac = 0.2)
```
Here, `n_iter`, `n_layers`, `n_neurons` and `test_frac` are the number of training iterations, the number of layers (not counting the input layer), the number of neurons per hidden layer, and the fraction of data that is reserved for testing purposes. Here, we reserve the last 20% of the data, which is therefore not used for training. These are all standard, and should normally be chosen by a suitable hyperparameter selection scheme, e.g. a grid search. The `d` hyperparameter described above is specific to DAS surrogates, and determines the dimension of the active subspace. We will illustrate its role in the examples below.

### Example 1: D = 2

In this example we will consider a low-dimensional problem with just 2 uncertain inputs. Normally we would not construct a DAS surrogate for `D=2`, but it does allow us to nicely visualize the concept of finding directions in the input space along which most of function variability occurs, which is the main idea behind active subspaces. If we select `d=1`, we are looking for the single most important direction in the 2D input space. In this artificial example we have som control over which inputs are important via the specification of the `a` vector. If we set `a_1=0` and `a_2=99`, we are creating a model in which virtually all the variability is aligned with the coordinate axis of the `x_1` input. In two dimensions we can plot the contour lines of the G function, as well as the coordinate vector of the 1D active subspace. Remember from the theoretical description that the active-subspace coordinate vectors are the column vectors of the weight matrix `W_1(Q)`. This weight matrix can be accessed via `w = surrogate.neural_net.layers[1].W`. The results for the case `a_1=0` and `a_2=99` are shown below.

![](contours1.png)

Clearly, the function does not change in `x_2` direction, and is therefore effectively one dimensional. The blue arrow denotes the (only) column vector of `W_1(Q)` after training, which is aligned with the `x_1` coordinate axis. 

Now imagine a situation where `x_2` also influences the solution, e.g. `a_1 = 0` and `a_2 = 1.0`. In this case `x_1` is still more important, yet there is no more 1D active subspace, as can be seen from the contours:

![](contours2.png)

In this case the DAS surrogate fails to properly converge, and the active subspace is still (incorrectly) aligned with the `x_1` axis. Increasing `d` from 1 to 2 yields the following result

![](contours3.png)

This yields an accurate surrogate (relatively low test error, see `tests/deep_active_subspaces/train_DAS_surrogate.py`). Again however, since `d=D` we can no longer speak of an active subspace. Other problems might very well have an active subspace. Consider for instances the following function:

![equation](https://latex.codecogs.com/svg.latex?f%28%7B%5Cbf%20x%7D%29%20%3D%20%5Cprod_%7Bi%3D1%7D%5ED%20%5Cfrac%7Ba_ix_i%5E2%20&plus;%201%7D%7B2%5ED%7D)

The `a_i` coefficients play a similar role as before, except that smaller values result in a less significant input. The results, for `a_1=1`, `a_2=0.5` and `d=1` are as follows:

![](contours4.png)

Here we obtain an accurate surrogate, and it is clear that the DAS surrogate was able to find a single dominant direction along which most of the variability takes place.
