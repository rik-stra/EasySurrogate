# Tutorial: Training using a data frame generated by EasyVVUQ

In this tutorial we will create a forward uncertainty-propagation surrogate using a Deep Active Subspace network, trained on a data frame generated by [EasyVVUQ](https://github.com/UCL-CCS/EasyVVUQ). Forward uncertainty propagation is defined as computing the output distribution of a computational model, given *assumed* probability density functions for the input paramaters of the model, see the image below for a sketch of the problem. A related problem is creating a cheap surrogate model for the input-output map, which can be evaluated at a fraction of the cost of the computational model. EasyVVUQ is VECMA's forward uncertainty propagation toolkit. 

![](forward.png)

## Problem definition

For computational efficiency we will consider the analytical Sobol G function, see [this page](https://www.sfu.ca/~ssurjano/gfunc.html) for a desription of this function. The following files are relevant:

* `tests/deep_active_subspaces/model/g_func.py`: the implementation of the Sobol G function.
* `tests/deep_active_subspaces/generate_easyvvuq_dataframe.py`: script that generates the training data using EasyVVUQ.
* `tests/deep_active_subspaces/train_DAS_surrogate.py`: script that trains a Deep Active Subspace (DAS) surrogate on the EasyVVUQ data frame.

## Deep Active subspaces

The Active Subspace method, introduced in [1], is a class of methods for forward propagation of uncertainty in high-dimensional input spaces. It attempts to circumvent the curse of dimensionality by
dimension reduction of the input space. Specifically, it projects the input vector x (of size `D`) to a lower-dimensional subspace y of size `d` (`d < D`), via a tall-and-skinny matrix `W_1` of orthogonal basis vectors. The active subspace is thus given by

![equation](https://latex.codecogs.com/svg.latex?%7B%5Cbf%20y%7D%20%3D%20W_1%5ET%7B%5Cbf%20x%7D%20%5Cin%5Cmathbb%7BR%7D%5Ed%2C)

## EasyVVUQ Latin Hypercube campaign

## Training on a EasyVVUQ data frame

The EasyVVUQ data frame can be read into EasySurrogate via:

```python
# Create an EasySurrogate campaign
surr_campaign = es.Campaign()

# This is the main point of this test: extract training data from EasyVVUQ data frame
features, samples = surr_campaign.load_easyvvuq_data(campaign, qoi_cols='f')
```

This will output:

```
Extracting features ['x1', 'x2', 'x3', 'x4', 'x5']
Extracting output data ['f'] 
```
indicating that it has read samples drawn 5 input variables, which will be used as features. The output data is in this case a single column of the CSV output file of the G function, named 'f'. If you which the read multiple columns, specify a list of names under the `qoi_cols` parameter of `load_easyvvuq_data`. Note that `features` is an array, wheres `samples` is a dictionary indexed by the `qoi_cols`. In this case, `samples['f']` will return an array with all outut values of the G function. From this point onward we can simply use this data as training data for any surrogate. Here, we will use it to train an ANN surrogate:

```python
# Create artificial neural network surrogate
surrogate = es.methods.ANN_Surrogate()

# Number of training iterations (number of mini batches)
N_ITER = 10000

# The latter fraction of the data to be kept apart for testing
TEST_FRAC = 0.3

# Train the ANN
surrogate.train(features, samples['f'], N_ITER,
                n_layers=4, n_neurons=50, test_frac=TEST_FRAC)

```

Note that here we reserved the latter 30 % of the data for testing the accuracy of the surrogate. We evaluate the training and test error via:

```python
# get some useful dimensions of the ANN surrogate
dims = surrogate.get_dimensions()

# evaluate the ANN surrogate on the training data
training_predictions = np.zeros([dims['n_train'], dims['n_out']])
for i in range(dims['n_train']):
    training_predictions[i] = surrogate.predict(features[i])

# print the relative training error
error_train = np.linalg.norm(training_predictions - samples['f'][0:dims['n_train']]) /\
    np.linalg.norm(samples['f'][0:dims['n_train']])
print("Relative error on training set = %.3f percent" % (error_train * 100))

# evaluate the ANN surrogate on the test data
test_predictions = np.zeros([dims['n_test'], dims['n_out']])
for count, i in enumerate(range(dims['n_train'], dims['n_samples'])):
    test_predictions[count] = surrogate.predict(features[i])

# print the relative test error
error_test = np.linalg.norm(test_predictions - samples['f'][dims['n_train']:]) /\
    np.linalg.norm(samples['f'][dims['n_train']:])
print("Relative error on test set = %.3f percent" % (error_test * 100))
```

Here `dims` is a dictionary containing the size of the training, testing and total data, as well as the number of in- and output neurons of the ANN. The output will look something like:

```
Relative error on training set = 0.294 percent
Relative error on test set = 2.944 percent
```
indicating that we have a training / test error of 0.3 % and 3 % respectively. Again, the entire script can be found here: `tests/easyvvuq_easysurrogate_coupling/coupling_example.py`.
